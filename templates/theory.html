{% extends "base.html" %}

{% block app_content %}
    <h1>Fundamental concepts</h1>
    <div id='fundamental_concepts'>
    <p>A <i>Boolean algebra</i> is a mathematical system $(\beta, +, \cdot)$ consisting of a nonempty
    set $\beta$ and two binary operations $+$ and $\cdot$ defined on $\beta$ such that (1) seach of the operations $+$
    and $\cdot$ is commutative; (2) each operation is distibutive over the other; (3) there exist distinct identity
    element $0$ and $1$ relative to operations $+$ and $\cdot$ respectively, that is, $a+0=a$, $a*1=a$ for all
    $a\in\beta$; (4) for each element $a\in\beta$, there exists an element $a^C\in\beta$, called <i>complement</i> of
    $a$, such that $a+a^C=1$, $a\cdot a^C=0$.</p>
    <p>We will primarily work with the two element Boolean algebra. We shall use $\beta_0$ to denote the
    set $\{0,1\}$ with three operations $+$, $\cdot$, $^C$ defined as follows: $0+0=0\cdot 1=1 \cdot 0 = 0\cdot 0=0$,
    $1+ 0=0+1=1+1=1\cdot 1=1$, $0^C=1$, and $1^C=0$.</p>
    <h3>Boolean vectors</h3>
    <div id='boolean_vectors'>
        <p><b>Definition 1.1.</b><span id='definition_1_1'> Let $V_n$ denote the set of all $n$-tuples $(a_1, a_2, ..., a_n)$ over $\beta_0$. An element of $V_n$ is called a <i>Boolean vector</i> of dimension $n$. The system $V_n$ together with the operation of component-wise addition is called the <i>Boolean vector space of dimension $n$</i>.</span></p>
        <p><b>Definition 1.2.</b><span id='definition_1_2'> Let $V^n=\{v^T: v \in V_n\}$, where $v^T$ we mean <i>column vector</i> $$\begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}$$</span></p>
        <p><b>Definition 1.3.</b><span id='definition_1_3'> Define the <i>complement</i> $v^C$ of $v$ to be the vector such that $v_i^C=1$ if and only if $v_i=0$, where $v_i$ denote the $i$the component of $v$.</span></p>
        <p><b>Definition 1.4.</b><span id='definition_1_4'> Let $e_i$ be the $n$-tuple with $1$ as $i$th coordinate, $0$ otherwise. Further, we define $e^i=e_i^T$.</span></p>
        <p><b>Definition 1.5.</b><span id='definition_1_5'> A $subspace$ of $V_n$ is a subset containing the zero vector and closed under addition of vectors. The $span$ of a set $W$ of vectors, denoted $\lt W\gt$, is the intersection of all subspaces containing $W$.</span></p>
        <p><b>Definition 1.6.</b><span id='definition_1_6'> Let $W\subset V_n$. A vector $v\in V_n$ is said to be <i>dependent</i> on $W$ if and only if $v\in\lt W\gt$. A set $W$ is said to be <i>independent</i> if ond only if for all $v\in W$, $v$ is not dependent on $W\setminus\{v\}$, where $\setminus$ denotes the set-theoretic difference. If $W$ is not independent we say that it is dependent.</span></p>
        <p><b>Definition 1.7.</b><span id='definition_1_7'> Let $u, v \in V_n$, the we say that $u\le v$ if and only if $v_i=1$ for each $i$ such that $u_i=1$. We say that $u\lt v$ if $u \le v$ and $u \ne v$.</span></p>
        <p><b>Definition 1.8.</b><span id='definition_1_8'> Let $W \subset V_n$. A subset $B$ of $W$ is called a <i>basis</i> of $W$ if and only if $W = \lt B \gt$, and $B$ is an independent set.</span></p>
        <p><b>Theorem 1.1.</b><span id='theorem_1_1'> Let $W$ be a subspace of $V_n$. Then there exists one subset $B$ of $V_n$ such that $B$ is a basis of $W$.</span></p>
        <p><b>Proof.</b><span id='proof_1_1'> Let $B$ be the set of all vectors of $W$ that are not sums of vectors of $W$ smaller then themselves. Then $B$ is an independent set. Suppose $B$ generates a proper subspace of $W$. The let $v$ be a minimal vector of $W \setminus \lt B \gt$. Then $v$ is expressible as a sum of smaller vectors since it is not in $B$. But these smaller vectors must be in $\lt B \gt$ since $v$ was minimal. Thus $v$ belongs in the subspace generated by $B$. This is a contradiction. Thus $B$ generates $W$ and is a basis. By independence, $B$ must be contained in every basis. Let $B'$ be another basis and let $u$ be a minimal element of $B'\setminus B$. Then by the reasoning above $u$ is dependent. This contradiction shows $B'=B$. This proves the theorem. &#9647;</span></p>
    </div>
    <h3>Boolean Matrices</h3>
    <div id='boolean_matrices'>
        <p><b>Definition 2.1.</b><span id='definition_2_1'> By a <i>Boolean matrix</i> of size $m \times n$ is meant an $m \times n$ matrix over $\beta_0$. Let $B_{mn}$ denote the set of all $m \times n$ such matrices. If $m=n$, we write $B_n$. Elements of $B_{mn}$ are often called <i>relation matrices</i>, <i>Boolean relation matrices</i>, <i>binary relation matrices</i>, <i>binary Boolean matrices</i>, <i>(0, 1)-Boolean matrices</i>, and <i>(0,1)-matrices</i>.</span></p>
        <p><b>Definition 2.2.</b><span id='definition_2_2'> Let $A = (a_{ij}) \in B_{mn}$. Then the element $a_{ij}$ is called the <i>$(i,j)$-entry</i> of $A$. The $(i,j)$-entry of $A$ is sometimes designated by $A_{ij}$. The $i$th <i>row</i> of $A$ is the sequence $a_{i1}, a_{i2}, ..., a_{in}$, and $j$th <i>column</i> of $A$ is the sequence $a_{1j}, a_{2j}, ... a_{mj}$. Let $A_{i*}$ ($A_{*i}$) denote the $i$th row (column) of $A$.</span></p>
        <p><b>Definition 2.3.</b><span id='definition_2_3'> The $n \times m$ <i>zero matrix $0$</i> is the matrix all of whose entries are zero. The $n \times n$ <i>identity matrix $I$</i> is the matrix $(\delta_{ij})$ such then $\delta_{ij}=1$ if $i=j$ and $\delta_{ij}=0$ if $i \ne j$. The $n \times m$ <i>universal matrix $J$</i> is the matrix all of whose entries are $1$.</span></p>
        <p><b>Definition 2.4.</b><span id='definition_2_4'> We use the notation $A^2$ to designate the product $AA$, $A^3=AA^2$, and in general $A^k=AA^{k-1}$ for any positive integer $k$. The matrix $A^k$ is called the $k$th <i>power</i> for obvious reasons. The motations $a^{(k)}_{ij}$ and $A^{(k)}_{ij}$ means $(i,j)$-entry and $(i,j)$-block of $A^k$. The notation $(A_{ij})^k$ means the $k$th power of the $(i,j)$-block of $A$.</span></p>
        <p><b>Definition 2.5.</b><span id='definition_2_5'> A <i>binary relation</i> on a set $X$ is a subset of $X \times X$. The <i>composition</i> of two binary relations $\rho_1$, $\rho_2$ is the raltion $\gamma$ such that $(x,y) \in \gamma$ if and only if for some $z$ both $(x,z) \in \rho_1$ and $(z,y) \in \rho_2$.</span></p>
        <p><b>Definition 2.6.</b><span id='definition_2_6'> The <i>adjacency matrix $A_G$</i> of a directed graph (digraph) $G$ is the matrix $A_G=(a_{ij})$ such that $a_{ij}=1$ if there is an arc from vertex $V_i$ to vertex $v_j$ and $a_{ij}=0$ otherwise. Dually, a digraph $G$ determines and is determined by the Boolean matrix $A_G$.</span></p>
        <p><b>Definition 2.7.</b><span id='definition_2_7'> A square matrix is called a <i>permutation matrix</i> if every row and every column contains exactly one $1$. Let $P_n$ denote the set of all $n \times n$ such matrices.</span></p>
        <p><b>Definition 2.8.</b><span id='definition_2_8'> The <i>transpose</i> of a Boolean matrix is obtained by rewriting its rows and columns. The transpose of $A$ will be denoted by $A^T$.</span></p>
        <p><b>Definition 2.9.</b><span id='definition_2_9'> A matrix is said to be a <i>partial permutation matrix</i> if every row and every column of it contains at most one $1$.</span></p>
        <p><b>Definition 2.10.</b><span id='definition_2_10'> Let $A, B \in B_{mn}$. By $B \le A$ we mean if $b_{ij}=1$ then $a_{ij}=1$ for every $i$ and $j$.</span></p>
        <p><b>Definition 2.11.</b><span id='definition_2_11'> The <i>row space</i> of a matrix $A$ is the span of the set of all rows of $A$. Likewise one has a <i>column space</i>. Let $R(A)$ ($C(A)$) denote the row (column) space of $A$.</span></p>
        <p><b>Definition 2.12.</b><span id='definition_2_12'> By a <i>partial order relation</i> on a set $X$ we mean a reflexive, antisymmetric and transitive relation on $X$. A set $X$ together with a specific partial order relation $P$ in $X$ is called a partially ordered set (poset). A <i>linear order</i> (also called <i>total order</i>) is a partial order relation $P$ such that for all $x,y$ $(x,y) \in P$ or $(y,x) \in P$.</span></p>
        <p><b>Definition 2.13.</b><span id='definition_2_13'> A <i>lattice</i> is a partially ordered set in which every pair of elements has a least upper bound (join) and greatest lower bound (meet). The operations join and meet are denoted by $\lor$ and $\land$ respectively.</span></p>
        <p><b>Definition 2.14.</b><span id='definition_2_14'> A lattice is said to be <i>distributive</i> if and only if $A \lor (B \land C) = (A \lor B) \land (A \lor C)$ for all $A, B, C$. This is equivalent to the dual condition $A \land (B \lor C)=(A \land B) \lor (A \land C)$.</span></p>
        <p><b>Proposition 2.1.</b><span id='proposition_2_1'> If $A \in B_{mn}$, then $R(A)$ ($C(A)$) is a subspace of $V_n$ ($V^m$).</span></p>
        <p><b>Proof.</b><span id='proof_2_1'> This relation holds for matrices over any semiring, and the proof is the same as in the case of matrices over $\mathbb{R}$. &#9647;</span></p>
        <p><b>Proposition 2.2.</b><span id='proposition_2_2'> Let $A \in B_{mk}$, $B \in B_{kn}$, then $R(AB) \subseteq R(B)$ and $C(AB) \subseteq C(A)$.</span></p>
        <p><b>Proof.</b><span id='proof_2_2'> The proof follows immediately from the fact that the rows of $AB$ are sums of the rows of $B$, etc. &#9647;</span></p>
        <p><b>Theorem 2.3.</b><span id='theorem_2_3'> If $A \in B_{mn}$, then $\lvert C(A) \rvert = \lvert R(A) \rvert$.</span></p>
        <p><b>Proof.</b><span id='proof_2_3'> We shall construct a bijection between $C(A)$ and $R(A)$. Let $v \in C(A)$, then there exists a unique set $\underline{s} \subset \underline{m} \subseteq \underline{n}$ such that $$v = \sum_\underline{s} e^i$$ Let $\underline{s}' = \underline{m} \setminus \underline{s}$ (we will use $'$ to denote set complements in this proof). Consider the map $f: C(A) \rightarrow R(A)$ given by $$f(v) = \sum_{\underline{s}'} A_{i*}$$ where $v \in C(A)$. Clearly $f$ is well-defined. We claim that the following statements are true.<br><br>
        (1) $f$ is injective: suppose we have $v,w \in C(A)$, $v \neq w$, and $$v = \sum_\underline{s} e^i $$ while $$w = \sum_\underline{t} e^i$$ where $\underline{t} \subset \underline{m}$ and that $f(v) = f(w)$, i.e., $$\sum_{s'} A_{i*} = \sum_{t'} A_{i*}$$ Since $v \neq w$, we may assume that there exist a $p \in \underline{t} \setminus \underline{s}$. But since $w \in C(A)$ there exists a $k \in \underline{n}$ such that $a_{pk}=1$ and $A_{*k} \leq w$. Since $p \in \underline{s}'$, we must have $(f(v))_k=1$, which implies that there exists a $q \in \underline{t}'$ such that $a_{qk}=1$. But since $A_{*k} \leq w$, this implies that $e^q \leq w$, which is impossible since $q \in t'$. Thus $f(v) \neq f(w)$.<br><br>
        (2) Since there exists an injection from the row space into the column space, we are through as it follows that $f$ is surjective. &#9647;</span></p>
        <p><b>Corollary 2.4.</b><span id='corollary_2_4'> Let $A$ and $f$ be as in Theorem 2.3, and let $v,w \in C(A)$. Then $v \leq w$ if and only if $f(v) \geq f(w)$.</span></p>
        <p><b>Proof.</b><span id='proof_2_4'> <b>Necessity.</b> Clear, in that $\underline{s} \subset \underline{t}$ if and only if $\underline{s}' \supset \underline{t}'$. <b>Sufficiency. </b>This follows from the same proof as that given for injectivity of $f$. Assume $w \nleq v$ but $f(w) \geq f(v)$, and follow exactly the same procedure. &#9647;</span></p>
        <p><b>Proposition 2.5.</b><span id='proposition_2_5'> Let $A_1, A_2, ..., A_k \in B_n$ and let $B = A_1 A_2 ... A_k$. Then $\lvert C(B)\rvert = \lvert R(B) \rvert \leq \lvert R(A_i) \rvert = \lvert R(A_I) \rvert = \lvert C(A_i) \rvert$ for all $i$.</span></p>
        <p><b>Proof.</b><span id='proof_2_5'> For any $M, N$ we have $\lvert R(MN) \rvert \leq \lvert R(N) \rvert$ and $\lvert R(MN) \rvert = \lvert C(MN) \rvert \leq \lvert C(M) \rvert = \lvert R(M) \rvert$, by Proposition 2.2. The present proposition follows from this by induction. &#9647;</span></p>
        <p><b>Definition 2.15.</b><span id='definition_2_15'> Let $A \in B_{mn}$. By $B_r(A)$ we mean the unique basis of $R(A)$, and we call it the <i>row basis</i> of $A$. Similarly, by $B_c(A)$ we mean the unique basis of $C(A)$, which we call the <i>column basis</i> of $A$. The cardinality of $B_r(A)$ ($B_c(A)$) is called the <i>row (column) rank</i> of $A$ and is denoted by $\rho_r(A)$ ($\rho_c(A)$).</span></p>
    </div>
    <h3>Green's Relations</h3>
    <div id='greens_relations'>
        <p><b>Definition 3.1.</b><span id='definition_3_1'> A <i>right (left) ideal</i> in a semigroup $S$ is a subset $X$ such that $XS \subseteq X$ ($SX \subseteq X$), and the <i>(two sided) ideal</i> of $S$ generated by $X$ is $SXS \cup XS \cup SX$. <i> Principal ideals</i>, <i>principal left</i> and <i>right ideals</i> are defined in a similar way.</span></p>
        <p><b>Definition 3.2.</b><span id='definition_3_2'> Two elements of a semigroup $S$ are said to be <i>$\mathcal{L}$-equivalent</i> if they generate the same pricipal left ideal of $S$. <i>$\mathcal{R}$-equivalence</i> is defined dually. The join of the equivalence relations $\mathcal{L}$ and $\mathcal{R}$ is denoted by $\mathcal{D}$ and their intersection by $\mathcal{H}$. Two elements are said to be <i>$\mathcal{J}$-equivalent</i> if they generate the same two-sided pricipal ideal. These five relations are known as <i>Green's relations</i>.</span></p>
        <p><b>Definition 3.3.</b><span id='definition_3_3'> The <i>weight</i> of a vector $v$, denoted by $w(v)$, is the number of nonzero elements of $v$. The weight of $v$ is sometimes called the rank of the vector $v$.</span></p>
        <p><b>Lemma 3.1.</b><span id='lemma_3_1'> Two matrices $A, B$ are $\mathcal{L}$ ($\mathcal{R}$)-equivalent if and only if they have the same row (column) space.</span></p>
        <p><b>Proof.</b><span id='proof_3_1'> Suppose $XA=B$ and $YB=A$. Then $R(B) \subseteq R(A)$ and $R(A) \subseteq R(B)$ so $R(A) = R(B)$. Suppose $R(B) \subseteq R(A)$. Then by looking at each row of $B$ we can find an $X$ such that $XA=B$. Likewise we can find a $Y$ such that $YB=A$. &#9647;</span></p>
        <p><b>Lemma 3.2.</b><span id='lemma_3_2'> Let $U$ be any subspace of $V_n$ and $f$ a homomorphism of commutative semigroups from $U$ into $V_n$ such that $f(0)=0$. Then there exists a matrix $A$ such that for all $v \in V_n$, $vA = f(v)$.</span></p>
        <p><b>Proof.</b><span id='proof_3_2'> Let $S(i)=\{v \in U; v_i = 1 \}$. Then define $A_{i*}$ to be $inf \{ f(v): v \in S(i)\}$. We will show $vA=f(v)$, for all $v \in U$. Suppose $(vA)_j=1$. Then for some $k$, $v_k=1$ and $a_{kj}=1$. Thus for all $w \in S(k)$, $(f(w))_j=1$. Since $v \in S(k)$, $(f(v))_j=1$. This proves that $vA \le f(v)$.<br><br>Suppose that $(vA)_j=0$. Then for all $k$ such that $v_k=1$, we have $a_{kj}=0$. Thus for all $k$ such that $v_k=1$ we have a vector $x(k) \in S(k)$ such that $f(x(k))_j=0$. But $(\sum x(k))_k$ is $1$ for each $k$ such that $v_k=1$. Thus $\sum x(k) \ge v$. Thus $\sum f(x(k)) = f(\sum x(k)) \ge f(v)$. Since $f(x(k))_j=0$ for each $k$, $f(v)_j = 0$. This proves $vA \ge f(v)$. &#9647;</span></p>
        <p><b>Theorem 3.3.</b><span id='theorem_3_3'> Two matrices in $B_n$ belong in the same $\mathcal{D}$-class if and only if their row spaces are isomporphic as lattices.</span></p>
        <p><b>Proof.</b><span id='proof_3_3'> The row space of matrix is the same as its image space on row vectors. And two such spaces are isomorphic as lattices if and only if they are isomorphic as commutative semigroups.<br><br>Suppose $A \mathcal{D} B$. Let $C$ be such that $A \mathcal{L} C$ and $C \mathcal{R} B$. Then $A$, $C$ have identical image spaces on row vectors. Let $X$, $Y$ be such that $CX=B$ and $BY=C$. Then we have maps $f: V_n C \rightarrow V_n B$ and $g: V_n B \rightarrow V_n C$ given by multiplying on the right by $X$ and $Y$. We have $fg$ and $gf$ are the identity. Thus the image spaces of $B$ adn $C$ are isomorphic. Thus if $A \mathcal{D} B$, $R(A) \simeq R(B)$.<br><br>Suppose $R(A) \simeq R(B)$. Let $h$ be an isomorphism from $V_n A$ to $V_n B$. By Lemma 3.2 we have matrices matrices $X$, $Y$ such that $vX=h(v)$ for $v \in V_n A$ and $vY = h^{-1}(v)$ for $v \in V_n B$. Thus $XY$ is the identity on $V_n A$ and $YX$ is the identity on $V_n B$. Then $A=AXY$ and $V_nAX=V_n B$. Thus $A \mathcal{R} AX$ and $AX \mathcal{L} B$. So $A \mathcal{D} B$. This proves the theorem. &#9647;</span></p>
        <p><b>Definition 3.4.</b><span id='definition_3_4'> Let $S$ be a semigroup, and let $a \in S$. We define: $L_a=\{b \in S: a \mathcal{L} b\}$, $R_a = \{b \in S: a \mathcal{R} b\}$, $H_a = \{b \in S: a \mathcal{H} b\}$, $D_a = \{b \in S: a \mathcal{D} b\}$, $J_a = \{b \in S: a \mathcal{J} b\}$.</span></p>
        <p><b>Theorem 3.4.</b><span id='theorem_3_4'> Let $A \in B_n$. Then the elements of $H_a$ are in one-to-one correspondence with the lattice automorphisms of $R(A)$.</span></p>
        <p><b>Proof.</b><span id='proof_3_4'> Let $\alpha$ be an automorphism from $R(A)$ to $R(A)$. Then $A \alpha$ gives a linear map from $V_n$ to $V_n$ sending $0$ to $0$, i.e., a matrix $B$. The matrices $A$, $B$, both have image space $R(A)$, so they are $\mathcal{L}$-equivalent. By Lemman 3.2, there exist matrices $X$, $Y$ so that on $R(A)$ the equations $X=\alpha$ and $Y=\alpha^{-1}$ hold. Then for any vector $v$ it is true that $v A X = v A \alpha = vB$ and $v B Y = v B \alpha^{-1} = v A \alpha \alpha^{-1} = v A$. So $AX=B$ and $BY=A$. Thus $A \mathcal{H} B$. This defines a function from the automorphisms of $R(A)$ into the $\mathcal{H}$-class of $A$. Two different automorphisms will give rise to different maps $A \alpha$ and so to different matrices $B$. Thus the function is one-to-one. Let $B$ be any matrix in the $\mathcal{H}$-class of $A$. Then $R(A)=R(B)$ and there exist matrices $X$ and $Y$ such that $AX=B$ and $BY=A$. This implices that $X$ and $Y$ map $R(A)$ to itself and that $X$ gives an automorphism of $R(A)$. This proves the function is onto, and completes the proof of the theorem. &#9647;</span></p>
        <p><b>Lemma 3.5.</b><span id='lemma_3_5'> The number of permutations of $n$ objects with repetitions allowed wich may be formed from $p$ objects of which $k$ have been singled out to appear in every one of these permutations is $$\sum_{i=0}^k (-1)^i \begin{pmatrix}k \\ i \end{pmatrix} (p-i)^n$$</span></p>
        <p><b>Proof.</b><span id='proof_3_5'> We will prove the lemma by using generating functions for permutations although the lemma can be proved directly by analyzing which kinds of permutations are permissible under the rule stated in the lemma.<br><br>The generating function for the situation as described in the above will be $$(1+ t + {t^2 \over 2!} + ...)^{p-k} = (e^t-1)^k (e^t)^{p-k} \sum_{i=0}^k (-1)^i \begin{pmatrix} k \\ i \end{pmatrix} e^{(k-i) t} e^{(p-k) t} = \sum_{i=0}^k (-1)^i \begin{pmatrix} k \\ i \end{pmatrix} e^{(p-i) t}$$ Thus this equation can be written as $$\sum_{n=0}^\infty \sum_{i=0}^k (-1)^i \begin{pmatrix} k \\ i \end{pmatrix} (p-i)^n {t^n \over n!}$$ Once we have the generating function, the answer to our problem is the coefficient of $t^n/n!$, which turns out to be $$\sum_{i=0}^k (-1)^i \begin{pmatrix} k \\ i \end{pmatrix} (p-i)^n$$ This completes the proof. &#9647;</span></p>
        <p><b>Corollary 3.6.</b><span id='corollary_3_6'> For any integer $n$, $$n! = \sum_{i=0}^n (-1)^i \begin{pmatrix} n \\ i \end{pmatrix} (n-i)^n.$$</span></p>
        <p><b>Proof.</b><span id='proof_3_6'> The right hand side of the equality written above is the number of permutations of $n$ objects of which $n$ have been singled out to appear of all permutations of $n$ objects without repetitions. &#9647;</span></p>
        <p><b>Theorem 3.7.</b><span id='theorem_3_7'> If $A \in B_n$, $\rho_r(A)=s$, $\rho_c(A)=t$, $\lvert H_A \rvert = h$, and $\lvert R(A) \rvert = \lvert C(A) \rvert \rvert = p$, then <br><br>(1) $\vert L_A \rvert = \sum_{i=0}^s (-1)^i \begin{pmatrix} s \\ i \end{pmatrix} (p-i)^n$, $\lvert R_A \rvert = \sum_{i=0}^t (-1)^i \begin{pmatrix} t \\ i \end{pmatrix} (p-i)^n$ (2) the number of $\mathcal{L}$-classes in $D_A$ is $h^{-1} \lvert R_A \rvert$ and similarly the number of $\mathcal{R}$-classes in $D_A$ is $h^{-1} \lvert L_A \rvert$. The number of $\mathcal{H}$-classes equals the number of $\mathcal{L}$-classes multiplied by the number of $\mathcal{R}$-classes or $h^{-2} \lvert L_A \rvert \lvert R_A \rvert$ (3) $\lvert D_A \rvert = h$ (number of $\mathcal{L}$-classes) (number of $\mathcal{R}$-classes $= h^{-1} \lvert L_A \rvert \lvert R_A \rvert$</span></p>
        <p><b>Proof.</b><span id='proof_3_7'> Follows directly from structure of Green's equivalence class and Lemma 3.5. &#9647;</span></p>
        <p><b>Corollary 3.8.</b><span id='corollary_3_8'> Let $A \in B_n$ and $\rho_r(A) = \rho_c(A)$. Then $\lvert L_A \rvert = \lvert R_A \rvert$ and the number of $\mathcal{L}$-classes in $D_A$ equals the number of $\mathcal{R}$-classes in $D_A$.</span></p>
        <p><b>Theorem 3.9.</b><span id='theorem_3_9'> For a matrix $A$ let $A_0$ be the matrix obtained from $A$ by replacing all dependent rows and columns of $A$, and all but one copy of each independent row and column of $A$, by zero. Then $A \mathcal{D} A_0$ so $H_A = \lvert H_{A_0} \rvert$. And $H_{A_0} = \{P A_0: P A_0 = A_0 Q$ for some permutation matrices $P, Q\}$.</span></p>
        <p><b>Proof.</b><span id='proof_3_9'> There is a natural isomorphism between the row sapce of $A$ and that of $A_0$, so $A$ and $A_0$ are $\mathcal{D}$-equivalent. Thus $\lvert H_A \rvert = \lvert H_{A_0} \rvert$. The indicated set of matrices are both $\mathcal{R}$ and $\mathcal{L}$-equivalent to $A_0$ so they lie in $H_{A_0}$.<br><br>Conversely let $X \in H_{A_0}$. Then $X$ has the same row space and the same column space as $A_0$. Suppose $X_{i*} \neq 0$ but $(A_0)_{i*}=0$. Then some column of $X$ has its $i$-entry nonzero. But no column of $A$ has $i$-entry nonzero. This is a contradiction.<br><br>Thus $X$ can have no more nonzero rows than $A_0$. But $X$ must contain all the nonzero rows of $A_0$, since the nonzero rows are distinct basis vectors. Thus the rows of $X$ are permutations of the rows of $A_0$. Thus $X = P A_0$ for some permutation matrix $P$. Likewise $X = A_0 Q$ for some permutation matrix $Q$. This proves the theorem. &#9647;</span></p>
        <p><b>Corollary 3.10.</b><span id='corollary_3_10'> If $A$ has row or column rnak $r$ then $\lvert H_A \rvert$ divides $r!$</span></p>
        <p><b>Theorem 3.11.</b><span id='theorem_3_11'> Let $A, B \in B_n$. (1) If $A \mathcal{L} B$, then $\lvert C(A) \rvert = \lvert C(B) \rvert$. (2) If $A \mathcal{R} B$, then $\lvert R(A) \rvert = \lvert R(B) \rvert$. (3) If $A \mathcal{D} B$, then $\lvert R(A) \rvert = \lvert R(B) \rvert = \lvert C(B) \rvert = \lvert C(B) \rvert = \lvert C(A) \rvert$.</span></p>
        <p><b>Proof.</b><span id='proof_3_11'> Obvious from Theorem 2.3 and Lemma 3.1 &#9647;</span></p>
        <p><b>Devinition 3.5.</b><span id='devinition_3_5'> A family $\underline{m}$ of subsets of $\underline{n}$ is an <i>additive space</i> if $\emptyset \in \underline{m}$ and $\underline{s} \cup \underline{t} \in \underline{m}$ where $\underline{s}, \underline{t} \in \underline{m}$. Two such families are isomorphic if and only if they are isomorphic as semigroups udner addition.</span></p>
        <p><b>Definition 3.6.</b><span id='definition_3_6'> A lattice is of <i>type $(n,m)$</i> if it occurs as some subspace of $V_n$ which is generated (except for $0$) under union by $m$ elements. (This is equivalent to saying the lattice has at most $m$ generators toher than $0$ under union and the lattice has $n$ generatiors other then the highest element under intersection).</span></p>
        <p><b>Lemma 3.12.</b><span id='lemma_3_12'> Let $n$, $m$ tend to infinity in such a way that $${\log n \over m} \rightarrow 0, {\log m \over n} \rightarrow 0$$ Then the proportion of $m \times n$ matrices which have both row rank $m$ and column rank $n$ tends to $1$.</span></
        <p><b>Proof.</b><span id='proof_3_12'> Let $r(i, j)$ denote the number of matrices with $A_{i*} \ge A_{j*}$ and $c(i, j)$ the number with $A_{*i} \ge A_{*j}$. Then for fixed $i \ne j$ we have $${r(i,j) \over k}=({3 \over 4})^m, {c(i, j) \over k} = ({3 \over 4})^m$$ where $k = 2 ^ {nm}$. Thus the number of matrices having no row greater than or equal to any other, and no column greater than or equal to any other is at least $$(1-(n^2-n) ({3 \over 4})^m - (m^2 - m) ({3 \over 4})^n) 2^{nm}$$ All these matrices have row rank $m$ and column rank $n$. Under the given hypotheses this number divided by $2^{mn}$ will tend to $1$. This completes the proof. &#9647;</span></p>
        <p><b>Lemma 3.13.</b><span id='lemma_3_13'> If $P$ or $Q$ have no more than $k$ cycles the number of solutions $X$ of $PXQ = X$ is no more than $2^{kn}$ or $2^{km}$, respectively.</span></p>
        <p><b>Proof.</b><span id='proof_3_13'> Let $P$ have no more than $k$ cycles. CHoose one row from each cycle, and specify it. This can be done in $2^{kn}$ ways, and these rows determine the rest. Similarly for $Q$. &#9647;</span></p>
        <p><b>Lemma 3.14.</b><span id='lemma_3_14'> If a permutation $P$ has at least $k$ cycles, it will fix at least $m-2(m-k)$ numbers from $\underline{m}$.</span></p>
        <p><b>Proof.</b><span id='proof_3_14'> If $i$ numbers are fixed. $i + 2 (k-i) \le m$. &#9647;</span></p>
        <p><b>Lemma 3.15.</b><span id='lemma_3_15'> Let a permutation group $G$ act on a set $S$ of letters. If for any element $g$ of $G$, $g$ fixes at least at $\lvert S \rvert - a$ letters, with $a > 0$, then there is a set of $\lvert S \rvert - 2a + 1$ letters fixed by every element of $G$.</span></p>
        <p><b>Proof.</b><span id='proof_3_15'> The aciton of $G$ on $S$ gives a linear representation $R$ of $G$ by permutation matrices. Let $o_1$, $o_2$, $...$, $o_k$, $o_{k+1}$, $...$, $o_{k+t}$ be the $G$-orbits contained in $S$, where $o_1$, $o_2$, $...$, $o_k$ contain only one element each, and the rest contain more than one element. Corresponding to this orbit decomposition we have a direct sum decomposition $R = R_1 \oplus R_2 \oplus ... R_K \oplus R_{k+1} \oplus ... \oplus R_{k+t}$. A theorem in group representation theory states that $$ \sum_{g \in G} \DeclareMathOperator{\Tr}{Tr}\Tr (g) = (k+t) \lvert G \rvert $$ But for any $g \in G$, $\Tr(g) \ge \lvert S \rvert - a$ and, assuming $a \ge 0$, $\Tr (\theta) > \lvert S \rvert - a$, where $\Tr (g)$ denotes the trace of $g$ and $\theta$ denotes the identity element of $G$. Thus $\lvert S \rvert - a < k + t$. Yest $\vert S \rvert \ge k + 2t$. Thus $$\lvert S \rvert - a < k + {\lvert S \rvert - k \over 2}$$ which yields the desired inequality on $k$. &#9647;</span></p>
        <p>From this lemma we derive an asymptotic bound on the number of $\mathcal{D}$-classes. Namely for two matrices of row rank $n$, if they are $\mathcal{L}$-equivalent they have the same row basis, but the row basis consists of all rows. Thus the rows of one matix are a permutation of the other matrix. Thus $A \mathcal{L} B$ if and only if $A=PB$ for a permutation matrix $B$. Likewise $A \mathcal{R} B$ if and only if $A=BQ$. It follows that $A \mathcal{D} B$ if and only if $A=PBQ$. Thus these $\mathcal{D}$-classes have at most $n!m!$ members. Thus the number of $\mathcal{D}$-classes of these row and column rank $n$ element is at least $${2^{nm}-o(2^{nm}) \over n!m!} = {2^{nm} \over n!m! } (1-O(1))$$ This gives the lower bound. To prove the upper bound, we will prove that the number of $D$-classes containing a matrix $X$ such that $PXQ=X$ for nonidentity permutation matrices $P, Q$ is $$o({2^{nm} \over n!m!})$$ This means most $D$-classes have at least $n!m!$ members. Thus the number of $\mathcal{D}$-classes cannot exceed $${2^{nm} \over n!m!}(1-o(1))$$</p>
        <p><b>Theorem 3.16.</b><span id='theorem_3_16'> Let $n$, $m$ tend to infinity in such a way that $n/m$ tends to a nonzero constant. Then the number of $\mathcal{D}$-classes of $m \times n$ Boolean matrices is asymptotically equal to $${2^{nm} \over n!m!}$$</span></p>
        <p><b>Proof.</b><span id='proof_3_16'> By Lemma 3.12 and the consideration after its proof we need only prove this formula gives an asymptotic upper bound.<br><br>Let $k=\sup\{\lim n/m, \lim m/n\}$. Case 1. $\mathcal{D}$-classes containing some $X$ such that $PXQ=X$ for some $P$, $Q$ such that $P$ has no more than $m-(4k+1)\log m$ cycles. (All logarithms are base 2). For fixed $P$, $Q$ with $P$ satisfying the hypothesis of this case, there are at most $$2^{(m-(4k+1)\log m)n}$$ matrices $X$ such that $PXQ=X$, by Lemma 1.3.13. The number of possibilities for $P$, $Q$ cannot exceed $n!m!$. Thus the number of possibilities for $X$ in the present case is at most $$2^{(m-(4k+1)\log m)n}n!m!$$ Thus also the number of $\mathcal{D}$-classes containing at least one such $X$ is at most $$2^{(m-(4k+1)\log m)n}n!m!$$ The ratio of this number to $${2^{nm} \over n!m!}$$ will approach zero.<br><br>Case 2. $\mathcal{D}$-classes containing some matrix $X$ such that $PXQ=X$ for some $P$, $Q$ such that $Q$ has no more than $n-(4k+1)\log n$ cycles. This case is treated like Case 1.<br><br>Case 3. $\mathcal{D}$-classes containing a matrix $X$ such that $PXQ=X$ for some $P$, $Q$ not both identity but such that $PXQ=X$ does not hold for any $P$, $Q$ with $P$ having no more that $m-(4k+1)\log m$ cycles or $Q$ having no more than $n-(4k+1)\log n$ cycles. For such an $X$, choose a pair $P$, $Q$ satisfying $PXQ=X$ such that $\sup \{m-$number of cycles in $P, n-$number of cycles in $Q\}$ is a maximum. Let $s$ denote this maximum. We have $0 < s < (4k+1)(\sup\{\log m, \log n\})$. For a given $X$ the set $\{P: PXQ=X $for some $Q\}$ forms a group. Each elemnt of this group will fix at least $m-2s$ letters by Lemma 1.3.14. Thus by Lemma 1.3.12 the whole group will fix at least $m-4s$ letters. There is a similar group of $Q$'s which fixes at least $n-4s$ letters.<br><br>Fix $s$. We first choose a set of $4s$ letters which is to contain the set of all non-fixed letter under $\{P: PXQ=X$ for some $Q\}$. There are $\begin{pmatrix}m \\ 4s \end{pmatrix}$ such choices. There is $\begin{pmatrix}n \\ 4s\end{pmatrix}$ choices for a similar set for $\{Q: PXQ=X$ for some $P\}$. Given that these sets are chosen, we can choose $P$ in $(4s)!$ ways to act on its set and $Q$ in $(4s)!$ ways to act on its set. Once $P$, $Q$ are chosen we can choose $X$ in at most $$2^{nm-s(\min\{n,m\})}$$ ways by Lemma 1.3.13. Thus for given $s$, there are at most $$\begin{pmatrix}m \\ 4s\end{pmatrix} \begin{pmatrix}m \\ 4s\end{pmatrix} (4s)! (4s)! 2^{nm - s (\min\{n, m\})}$$ choices of $X$ having the required value of $s$. However these $X$'s do not all lie in different $\mathcal{D}$-classes. For any permutation matrices $E$, $F$; $EXF$ lie in the $\mathcal{D}$-class and have the same value of $s$.<br><br>How many different matrixes $EXF$ are there for a given $X$? We have a group action of the product of two symmetric groups on such matrices, sending $Y$ to $EYF^{-1}$. Here $F^{-1}$ denotes the inverse of $F$. The isotropy group of $X$ has order at most $(4s)!(4s)!$ by the remarks above about choosing $P$, $Q$ such that $PXQ = X$. Thus $\mathcal{D}$-class containing one $X$ also contains at least $${n!m! \over (4s)! (4s)!}$$ other matrices with the same $s$. Thus the number of $D$-classes containing matrices of this type for given $s$ is at most $${m^{4s} n^{4s} 2^{nm - s (min\{n, m\})} (4s)! (4s)! \over n!m!}$$ Allowing any value of $s$ we have at most $$1 \le s \le (4k+1) n_1^{m^{4s} n^{4s} 2^{nm-sn} 2 ((4s)!(4s)!)(4k+1)\log n_1 \over n!m!}$$ where $n_1 = \max \{n, m\}$ and $n_2 = \min \{n, m \}$. The ratio of this quantity to $${2^{nm} \over n!m!}$$ tends to zero.<br><br>Case 4. All $PXQ$ are distinct so the $\mathcal{D}$-classes have at least $n!m!$ elements. There are at most $${2^{nm} \over n!m!}$$ $\mathcal{D}$-classes of this type. This proves the theorem. &#9647;</span></p>
        <p><b>Corollary 3.17.</b><span id='corollary_3_17'> Let $k$ be the number of matrices $X$ such that $PXQ=X$ for some $P$, $Q$ not both the identity. Then if $n, m \rightarrow \infty$ in such a way that $n \over m$ approaches a nonzero constant, $${k \over 2^{nm}}$$ approaches $0$.</span></p>
        <p><b>Theorem 3.18.</b><span id='theorem_3_18'> Under the hypotheses of Lemma 1.3.12, then the numbers of $\mathcal{D}$-classes of $\mathcal{D}$-classes of $B_{mn}$ are asymptotically equal to $${2^{nm} \over n!}, {2^{nm} \over m!}$$ respectively. The number of $\mathcal{H}$-classes is asymptotically equal to $2^{nm}$.</span></p>
        <p><b>Proof.</b><span id='proof_3_18'> For an upper bound, for instance for $\mathcal{R}$-classes we have $$\begin{pmatrix}2^m \\ n\end{pmatrix} + \begin{pmatrix}2^m \\ n-1\end{pmatrix} + ... + \begin{pmatrix}2^m \\ 1\end{pmatrix}$$ for column rank $k$, by choosing a set of $k$ column vectors to be a column basis. This is less than or equal to $$\begin{pmatrix}2^m \\ n\end{pmatrix} \sum_{i=1}^\infty ({n \over 2^m -1})^i$$ which gives the theorem. Similar methods apply in the other cases. This proves the theorem. &#9647;</span></p>
    </div>
    <h3>Regular Elements and Idempotents</h3>
    <div id='regular_elements_and_idempotents'>
    <p><b>Definition 4.1.</b>Let $a$, $b$ be elements of a semigroup $S$. We say that $a$ is a <i>regular element</i> of $S$ (or simply <i>regular</i>) if and only if there exists $x \in S$ such that $a=axa$. Let $Reg(X)$ denote the set of all regular elemnts contained in a set $X$.</p>
    <p><b>Definition 4.2.</b> Let $a$ be element of a semigroup $S$. We say that $a$ is <i>idempotent</i> if $a^2=a$. We note that if $a$ is idempotent, then it is regluar. Let $Idem(X)$ denote the set of all idempotents contained in a set $X$.</p>
    <p><b>Definition 4.3.</b>Two elements $a$ and $b$ of a semigroup $S$ are said to be $inverses$ of each other if $aba=a$ and $bab=b$</p>
    <p>By deleting dependent rows and columns from any idempotent we can obtain a $\mathcal{D}$-equivalent idempotent  which is <i>reduced idempotent</i> every nonzero row vector is a row basis vector. A $\mathcal{D}$-equivalent idempotent is the direct sum of a zero matrix and the matrix of a partial order relation.</p>
    <p><b>Proposition 4.1.</b> Let $a$ $b$ be elements of a semigroup $S$.</p>
    <p>(1) If $a \in Reg(S)$, then $b \in Reg(S)$ if $b \in D_a$.</p>
    <p>(2) If $a \in Reg(S)$, then every $L$-class and every $R$-class contained in $D_a$ contains an idempotent.</p>
    <p>(3) If $a \in Idem(S)$, then $xa=x$ for all $x \in L_a$, $ax=x$ for all $x \in R_a$ and $xa=x=ax$ for all $x \in H_a$.</p>
    <p>(4) If $a$ and $b$ are inverses of each other, then if we let $e=ab$ and $f=ba$ we have that $e, f \in Idem(S)$ such tat $ea=af=a$ and $be=fb=b$. Thus $e \in R_a \cap L_b$ and $f \in R_b \cap L_a$ and we have that $a, b, e, f \in D_a$.</p>
    <p>(5) If $e, f \in Idem(S)$ and $e \mathcal{H} f$, then $e = f$.
    <p><b>Proof.</b> (1) and (2) Let $x \in S$ be such that $axa = a$. Let $e = ax$ and $f = xa$. It follows that $e^2 = e$ and $f^2 = f$. By definition 3.1, $e \in R_a$ and $f \in L_a$, and so the $\mathcal{L}$-class and the $\mathcal{R}$-class of a regular element contain an idempotent. Let $b \in D_a$ and so there exists $x_1, y_1, x_2, y_2 \in S$ such that $b = x_1 a x_2$, $a = a x_2 y_2$, $a = x_1 y_1 a$. Consider $b(y_2xy_1)b=x_1(ax_2y_2)ax_2=x_1ax_2=b$, so that $b$ is regular.</p>
    <p>(3) If $x \in L_a$, then there exists $z \in S$ such that $za=x$, this implies $xa=(za)a=za=x$. The result for $R_a$ follows similarly and so we get the result for $H_a$.</p>
    <p>(4) If $a$ and $b$ are inverses of each other, then $a\mathcal{D}b$ by Theorem 3.3. The rest follow from Lemma 3.1.</p>
    <p>(5) Follows directly from (3) since we would have $a=ab=b$. This proves the proposition. &#9647;</p>
    <p><b>Lemma 4.2.</b> Let $a, b, x_1, y_1, x_2, y_2$ be elements of a semigroup $S$.<p>
    <p>(1) If $x_1a=b$ and $y_1b=a$ i.e., $a\mathcal{L}b$, then there exists an $\mathcal{L}$-class preservinc bijection between $R_a$ and $R_b$, i.e., all $\mathcal{R}$-classes contained in a $\mathcal{D}$-class have the same cardinality.</p>
    <p>(2) If $ax_a=b$ and $by_1=a$ i.e., $a\mathcal{R}b$, then there exists an $\mathcal{R}$-class preservinc bijection between $L_a$ and $L_b$, i.e., all $\mathcal{L}$-classes contained in a $\mathcal{D}$-class have the same cardinality.</p>
    <p>(3) $x_1ax_2=b$, $y_1x_1=a$, $ax_2y_2=a$, i.e., $a\mathcal{D}b$, then there exists a bijection between $H_a$ and $H_b$, i.e., all H-classes contained in a given $\mathcal{D}$-class have the same cardinality.</p>
    <p><b>Proof.</b> (1) Let $f: R_a \to S$ be given by $f(w)=x_1w$ for all $w \in R_a$. Since R is a left congruence $f(W) \in R_b$ (since $b=x_1a$). Thus we can actually consider $f$ to be a map into $R_b$. Similarly define $g: R_b \to R_a$ by $g(w) = y_1 w$. We claim $g \circ f$ is an identity on $R_a$. Let $w \in R_a$, then there exists a $z \in S$ such that $az=w$, and so $g(f(w))=g(f(az))=(y_1x_1a)z=az=w$. SImilarly $f \circ g$ is an identity on $R_b$. We have therefore shown the existence of a bijection between $R_a$ and $R_b$ and now we need only establish that $f$ is an $\mathcal{L}$-class preserving map. Let $w, z \in R_a$ be such that $w \in L_z$, then there must exist $t_1, t_2$ such that $t_1w=z$ and $t_2z=w$. But $f(w)=x_qw$ and $f(z)=x_1z$. Let $t'_1=x_1t_1y_1$ and let $t'_2=x_1t_2y_1$, then we have $t'_1f(w)=f(z)$ and $t'_2f(z)=f(w)$ since $g \circ f$ is identity. In essence this means that $f(H_a)=H_b$ and that in particular $|H_a|=|H_b|$.</p>
    <p>(2) The proof is similar to the proof of (q).</p>
    <p>(3) From the proofs of (1) and (2) above if $c \in S$ is such that $a \mathcal{L} c$ or $a \mathcal{R} c$, then there exists a bijection between $H_a$ and $H_b$. So (3) follows from this. This proves lemma. &#9647;</p>
    </div>
{% endblock %}
